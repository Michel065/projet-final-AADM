{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5d51be",
   "metadata": {},
   "source": [
    "# 2.1 Expérimentation avec l’algorithme approximative Nearest Neighbors (plus proches voisins approximatifs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41534868",
   "metadata": {},
   "source": [
    "### 2.1.1 Faire une synthèse de l’article avec au minimum 2 pages et un maximum de 3 pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef18e4",
   "metadata": {},
   "source": [
    "non fait ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495905fc",
   "metadata": {},
   "source": [
    "### 2.1.2 Construire un classeur binaire capable de classer les tweets en deux classes : positive et négative, selon les 4 scénarios suivants du dataset :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b1869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_csv = \"./data/source.csv\"\n",
    "\n",
    "# creation session scpark:\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"AADM\")\n",
    "    .config(\"spark.driver.memory\", \"12g\")\n",
    "    .config(\"spark.executor.memory\", \"12g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .config(\"spark.default.parallelism\", \"200\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d97100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "#On charger le CSV\n",
    "def load_csv(file_path):\n",
    "    df = spark.read.csv(file_path,header=False,inferSchema=True)\n",
    "    return df\n",
    "\n",
    "def nettoie_df(df,neutre = False,aff=False):\n",
    "    # on nomme les colones\n",
    "    df = df.toDF(\"label\", \"id\", \"date\", \"flag\", \"user\", \"text\")\n",
    "\n",
    "    #on allege:\n",
    "    df = df.select(\"id\",\"label\", \"text\")\n",
    "\n",
    "    #on filtre pour\n",
    "    #Nettoyer les données : suppression des mots non-pertinents, articles, urls. \n",
    "    if(neutre):\n",
    "        df = df.filter(col(\"label\").isin(0, 2 , 4))\n",
    "    else:\n",
    "        df = df.filter(col(\"label\").isin(0, 4))\n",
    "    df = df.withColumn(\"text\", regexp_replace(col(\"text\"), r\"http\\S+\", \"\"))\n",
    "    df = df.withColumn(\"text\", regexp_replace(col(\"text\"), r\"@\\w+\", \"\"))\n",
    "    df = df.withColumn(\"text\", regexp_replace(col(\"text\"), r\"[^a-z\\s]\", \"\"))\n",
    "    df = df.withColumn(\"text\", regexp_replace(col(\"text\"), r\"\\s+\", \" \"))\n",
    "    if(aff):\n",
    "        df.show(5, truncate=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba255cd5",
   "metadata": {},
   "source": [
    "execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c8699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_csv(data_file_csv)\n",
    "df_propre = nettoie_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0007742c",
   "metadata": {},
   "source": [
    "ensuite on creer une version du df avec des labels en binaire:\n",
    "\n",
    "fonction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1e96aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "def build_df_label_bin(df):\n",
    "    df = df.withColumn(\"label_bin\", when(col(\"label\") == 4, 1).otherwise(0)) # pour passer de 0-4 a 0-1\n",
    "    return df.select(\"id\",\"label_bin\", \"text\")\n",
    "\n",
    "# pour la question 2.1.10 on prend de l'avance\n",
    "def build_df_label_mc(df):\n",
    "    df = df.withColumn(\n",
    "        \"label_mc\",\n",
    "        when(col(\"label\") == 0, 0)\n",
    "        .when(col(\"label\") == 2, 1)\n",
    "        .when(col(\"label\") == 4, 2)\n",
    "    )\n",
    "    return df.select(\"id\", \"label_mc\", \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521b159",
   "metadata": {},
   "source": [
    "execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "099f1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_bin = build_df_label_bin(df_propre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ca271",
   "metadata": {},
   "source": [
    "2) On creer les scenarios\n",
    "\n",
    "fonctions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b68d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, explode, collect_list,coalesce, array,size\n",
    "from pyspark.ml.feature import Tokenizer, NGram, HashingTF\n",
    "\n",
    "num_features = 1<<18\n",
    "\n",
    "def make_df_Tokenizer(df_label):\n",
    "    tok = Tokenizer(inputCol=\"text\", outputCol=\"tokens\") #conversion en tokens\n",
    "    df_tok = tok.transform(df_label)\n",
    "    return df_tok.filter(size(col(\"tokens\")) > 0)#sinon ca crash lors des predictions\n",
    "\n",
    "def make_df_NGram(df,ngram_n=2,outputCol=\"ngrams\",inputCol=\"tokens\"):\n",
    "    ng = NGram(n=ngram_n, inputCol=inputCol, outputCol=outputCol)#creation des n grams\n",
    "    df_ng = ng.transform(df).filter(size(col(outputCol)) > 0)\n",
    "    return df_ng\n",
    "\n",
    "def make_s1_words(df,label_name=\"label_bin\"):\n",
    "    df_tok = make_df_Tokenizer(df)\n",
    "    tf = HashingTF(inputCol=\"tokens\", outputCol=\"features\", numFeatures=num_features, binary=True)\n",
    "    return tf.transform(df_tok).select(\"id\",label_name, \"features\")\n",
    "\n",
    "def make_s2_ngrams(df, ngram_n=2,label_name=\"label_bin\"):\n",
    "    df_tok = make_df_Tokenizer(df)\n",
    "    df_ng = make_df_NGram(df_tok,ngram_n)\n",
    "    tf = HashingTF(inputCol=\"ngrams\", outputCol=\"features\", numFeatures=num_features, binary=True)\n",
    "    return tf.transform(df_ng).select(\"id\",label_name, \"features\")\n",
    "\n",
    "def make_s3_patterns(df, ngram_n=2, topN=200000,label_name=\"label_bin\"):\n",
    "    df_tok = make_df_Tokenizer(df)\n",
    "    df_pat0 = make_df_NGram(df_tok, ngram_n, \"patterns\")\n",
    "\n",
    "    top = (df_pat0 #on creer le top\n",
    "        .select(explode(\"patterns\").alias(\"p\"))\n",
    "        .groupBy(\"p\").count()\n",
    "        .orderBy(col(\"count\").desc())\n",
    "        .limit(topN)\n",
    "        .select(\"p\")\n",
    "    )\n",
    "\n",
    "    df_pat = (df_pat0 #on filtre selon le top\n",
    "        .withColumn(\"p\", explode(\"patterns\"))\n",
    "        .join(top, on=\"p\", how=\"left_semi\")\n",
    "        .groupBy(\"id\", label_name)\n",
    "        .agg(collect_list(\"p\").alias(\"patterns_f\"))\n",
    "    )\n",
    "\n",
    "    tf = HashingTF(inputCol=\"patterns_f\", outputCol=\"features\", numFeatures=num_features, binary=True)\n",
    "    return tf.transform(df_pat).select(\"id\",label_name, \"features\")\n",
    "\n",
    "def make_s4_combo(df, ngram_n=2, topN=200000,label_name=\"label_bin\"):\n",
    "    #on refait les 3 senarios \n",
    "    df_tok = make_df_Tokenizer(df)\n",
    "\n",
    "    df_ng = make_df_NGram(df_tok, ngram_n, \"ngrams\")\n",
    "\n",
    "    df_pat0 = make_df_NGram(df_tok, ngram_n, \"patterns\")\n",
    "\n",
    "    top = (df_pat0\n",
    "        .select(explode(\"patterns\").alias(\"p\"))\n",
    "        .groupBy(\"p\").count()\n",
    "        .orderBy(col(\"count\").desc())\n",
    "        .limit(topN)\n",
    "        .select(\"p\")\n",
    "    )\n",
    "\n",
    "    df_pat = (df_pat0\n",
    "        .select(\"id\", label_name, \"patterns\")\n",
    "        .withColumn(\"p\", explode(\"patterns\"))\n",
    "        .join(top, on=\"p\", how=\"left_semi\")\n",
    "        .groupBy(\"id\", label_name)\n",
    "        .agg(collect_list(\"p\").alias(\"patterns_f\"))\n",
    "    )\n",
    "\n",
    "    df_all = (df_ng\n",
    "        .select(\"id\", label_name, \"tokens\", \"ngrams\")\n",
    "        .join(df_pat.select(\"id\", \"patterns_f\"), on=\"id\", how=\"left\")\n",
    "        .withColumn(\"patterns_f\", coalesce(col(\"patterns_f\"), array()))\n",
    "    )\n",
    "\n",
    "    df_combo = df_all.withColumn(\"combo\", concat(col(\"tokens\"), col(\"ngrams\"), col(\"patterns_f\"))).filter(size(col(\"combo\")) > 0)\n",
    "\n",
    "    tf = HashingTF(inputCol=\"combo\", outputCol=\"features\", numFeatures=num_features, binary=True)\n",
    "    return tf.transform(df_combo).select(\"id\",label_name, \"features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c171420",
   "metadata": {},
   "source": [
    "3) Construction d’un classifieur binaire pour la classification des tweets\n",
    "\n",
    "L’objectif est de construire un classifieur binaire capable de distinguer les tweets positifs et négatifs.\n",
    "Dans cette section, nous définissons l’ensemble des fonctions utilitaires qui seront utilisées dans les parties 2.1.2 à 2.1.9. Ces fonctions permettront de structurer le travail, de factoriser le code et de faciliter les expérimentations ultérieures.\n",
    "\n",
    "Nous commençons par définir une fonction permettant de séparer le jeu de données en ensembles d’entraînement et de test.\n",
    "\n",
    "Nous ajoutons également une fonction permettant de réduire la taille du dataset,pour la futur question 2.1.7 scalabilité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3308f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on split\n",
    "def split_train_test(df_feat, train_ratio=0.8, cache=True):\n",
    "    train_df, test_df = df_feat.randomSplit([train_ratio, 1 - train_ratio])\n",
    "    if cache:\n",
    "        train_df = train_df.cache()\n",
    "        test_df  = test_df.cache()\n",
    "        train_df.count()\n",
    "        test_df.count()\n",
    "    return train_df, test_df\n",
    "\n",
    "#on limite\n",
    "def limit_dataset_size(df, ratio=1.0):\n",
    "    if ratio >= 1.0:\n",
    "        return df\n",
    "    ratio = max(0,ratio)\n",
    "    return df.sample(withReplacement=False, fraction=ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab394f",
   "metadata": {},
   "source": [
    "ensuite on creer les fonctions qui vont faire l'Entraînement (MinHashLSH):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17dcd860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "import time\n",
    "# pour les focntions suivantes\n",
    "#  On recupere la construction dans l'exemple: LSHMinHash_datasetNetFlix.ipynb\n",
    "\n",
    "def fit_lsh(train_df, numHashTables, measure_time=False):\n",
    "    mh = MinHashLSH(\n",
    "        inputCol=\"features\",\n",
    "        outputCol=\"hashes\",\n",
    "        numHashTables=numHashTables\n",
    "    )\n",
    "    if(measure_time):\n",
    "        start = time.perf_counter()\n",
    "    \n",
    "    lsh_model = mh.fit(train_df)\n",
    "    lsh_model.transform(train_df).count() # test pas final pour voir si ca change le temps d'execution\n",
    "    \n",
    "    if measure_time: #c'est dans les cas plus tard ou on voudra le temps \n",
    "        end = time.perf_counter()\n",
    "        return lsh_model, float(end - start)\n",
    "    return lsh_model\n",
    "\n",
    "def train_models_for_scenario(df_feat, numHashTables_list=(128, 250), train_ratio=0.8, cache=True, measure_time=False):\n",
    "    train_df, test_df = split_train_test(df_feat, train_ratio=train_ratio, cache=cache)\n",
    "    models = {}\n",
    "    times  = {}\n",
    "    for nht in numHashTables_list:\n",
    "        if measure_time:\n",
    "            model, t = fit_lsh(train_df, numHashTables=int(nht), measure_time=True)\n",
    "            models[int(nht)] = model\n",
    "            times[int(nht)]  = t\n",
    "        else:\n",
    "            models[int(nht)] = fit_lsh(train_df, numHashTables=int(nht), measure_time=False)\n",
    "\n",
    "    if measure_time:\n",
    "            return models, train_df, test_df, times\n",
    "    return models, train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdff2a65",
   "metadata": {},
   "source": [
    "Fonctions pour prédiction AkNN + vote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f06942f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#Prédit en batch pour tout test_df via aevc vote majoritaire.\n",
    "def predict_knn(lsh_model, train_df, test_df, k, threshold=1.0,label_name=\"label_bin\"):\n",
    "    pairs = lsh_model.approxSimilarityJoin(\n",
    "        test_df,\n",
    "        train_df,\n",
    "        threshold=threshold,\n",
    "        distCol=\"JaccardDist\"\n",
    "    )\n",
    "\n",
    "    neigh = pairs.select(\n",
    "        F.col(\"datasetA.id\").alias(\"test_id\"),\n",
    "        F.col(f\"datasetA.{label_name}\").alias(\"true_label\"),\n",
    "        F.col(f\"datasetA.{label_name}\").alias(\"neighbor_label\"),\n",
    "        F.col(\"JaccardDist\")\n",
    "    )\n",
    "\n",
    "    w = Window.partitionBy(\"test_id\").orderBy(F.col(\"JaccardDist\").asc())\n",
    "    topk = neigh.withColumn(\"rank\", F.row_number().over(w)).filter(F.col(\"rank\") <= k)\n",
    "\n",
    "    votes = topk.groupBy(\"test_id\").agg(\n",
    "        F.first(\"true_label\").alias(\"true_label\"),\n",
    "        F.avg(\"neighbor_label\").alias(\"p_positive\")\n",
    "    )\n",
    "\n",
    "    pred_df = votes.withColumn(\"prediction\", (F.col(\"p_positive\") >= 0.5).cast(\"int\"))\n",
    "    return pred_df\n",
    "\n",
    "\n",
    "def evaluate_accuracy(pred_df):\n",
    "    \"\"\"\n",
    "    Calcule l'accuracy de pred_df (test_id, true_label, prediction).\n",
    "    \"\"\"\n",
    "    df_ok = pred_df.filter(F.col(\"true_label\").isNotNull() & F.col(\"prediction\").isNotNull())\n",
    "\n",
    "    row = (df_ok\n",
    "        .select((F.col(\"prediction\") == F.col(\"true_label\")).cast(\"int\").alias(\"ok\"))\n",
    "        .agg(F.avg(\"ok\").alias(\"acc\"))\n",
    "        .first()\n",
    "    )\n",
    "    acc = row[\"acc\"]\n",
    "    return float(acc) if acc is not None else 0.0\n",
    "\n",
    "def evaluate_k_grid(lsh_model, train_df, test_df, k_list=(50, 100, 150, 200), threshold=1.0,label_name=\"label_bin\"):\n",
    "    \"\"\"\n",
    "    Évalue l'accuracy pour plusieurs valeurs de k.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for k in k_list:\n",
    "        pred_df = predict_knn(lsh_model, train_df, test_df, k=k, threshold=threshold,label_name=label_name)\n",
    "        results[k] = evaluate_accuracy(pred_df)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9d612",
   "metadata": {},
   "source": [
    "Ensuite pour la futur question 2.1.5 on va vouloir un tableau pour comparer a l'article donc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f295aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def collect_results_row(scenario_name, numHashTables, k, accuracy):\n",
    "    return {\"scenario\": scenario_name,\"numHashTables\": int(numHashTables),\"k\": int(k),\"accuracy\": float(accuracy)}\n",
    "\n",
    "def build_results_table(results_rows, as_pandas=True):\n",
    "    \"\"\"\n",
    "    Construit le tableau de résultats.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(results_rows)\n",
    "    if as_pandas:\n",
    "        return df\n",
    "    return spark.createDataFrame(df)\n",
    "\n",
    "def format_table4_like(df_results):\n",
    "    \"\"\"\n",
    "    Met en forme le tableau\n",
    "    \"\"\"\n",
    "    return (df_results.pivot_table(index=[\"scenario\", \"numHashTables\"],columns=\"k\",values=\"accuracy\").sort_index())\n",
    "\n",
    "\n",
    "def eval_scenario(models, train_df, test_df, scenario_name,K_LIST = (50, 100, 150, 200),label_name=\"label_bin\"):\n",
    "    rows = []\n",
    "    for nht, model in models.items():\n",
    "        acc_by_k = evaluate_k_grid(model, train_df, test_df, k_list=K_LIST,label_name=label_name)\n",
    "        for k, acc in acc_by_k.items():\n",
    "            rows.append(collect_results_row(scenario_name, nht, k, acc))\n",
    "    print(f\"OK: {scenario_name} évalué\")\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740717ee",
   "metadata": {},
   "source": [
    "Fonction pour l'Histogramme pour la question 2.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d06d5b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def times_to_df(scenario_name, times_dict):\n",
    "    return pd.DataFrame([\n",
    "        {\"scenario\": scenario_name, \"numHashTables\": int(nht), \"train_time_seconds\": float(t)}\n",
    "        for nht, t in times_dict.items()\n",
    "    ])\n",
    "\n",
    "def plot_training_histogram(df_results, group_by=(\"scenario\",\"numHashTables\"), value_col=\"train_time_seconds\"):\n",
    "    dfp = df_results.copy()\n",
    "    dfp[\"group\"] = dfp[list(group_by)].astype(str).agg(\" | \".join, axis=1)\n",
    "\n",
    "    agg = dfp.groupby(\"group\")[value_col].mean()\n",
    "\n",
    "    plt.figure()\n",
    "    agg.plot(kind=\"bar\")\n",
    "    plt.xlabel(\"Modèle (scénario | numHashTables)\")\n",
    "    plt.ylabel(\"Temps d'entraînement (s)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d7880",
   "metadata": {},
   "source": [
    "Ensuite les fonctions pour la 2.1.7, la scalabilité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2043d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def benchmark_scalability_vs_fraction(df_feat,scenario_name,numHashTables,F_list=(0.2, 0.4, 0.6, 0.8, 1.0),train_ratio=0.8,cache=True):\n",
    "    rows = []\n",
    "    for F in F_list:\n",
    "        df_F = limit_dataset_size(df_feat, ratio=F)\n",
    "\n",
    "        train_df, _ = split_train_test(df_F, train_ratio=train_ratio, cache=cache)\n",
    "\n",
    "        _,t = fit_lsh(train_df, numHashTables=numHashTables,measure_time=True)\n",
    "\n",
    "        rows.append({\n",
    "            \"scenario\": scenario_name,\n",
    "            \"numHashTables\": int(numHashTables),\n",
    "            \"F\": float(F),\n",
    "            \"time_seconds\": t\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "#Plot Figure 3\n",
    "def plot_scalability_fraction(df_scaling, hue=\"scenario\"):\n",
    "    plt.figure()\n",
    "\n",
    "    if hue in df_scaling.columns:\n",
    "        for key, g in df_scaling.groupby(hue):\n",
    "            g2 = g.groupby(\"F\")[\"time_seconds\"].mean().sort_index()\n",
    "            plt.plot(g2.index, g2.values, marker=\"o\", label=str(key))\n",
    "        plt.legend()\n",
    "    else:\n",
    "        g2 = df_scaling.groupby(\"F\")[\"time_seconds\"].mean().sort_index()\n",
    "        plt.plot(g2.index, g2.values, marker=\"o\")\n",
    "\n",
    "    plt.xlabel(\"F (fraction de la taille originale)\")\n",
    "    plt.ylabel(\"Temps d'entraînement (s)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afadd85f",
   "metadata": {},
   "source": [
    "Pour la question 2.1.9, focntion simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7046132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_signatures(lsh_model, df_feat):\n",
    "    \"\"\"\n",
    "    Ajoute la colonne 'hashes' (signature MinHash) au DataFrame de features.\n",
    "    \"\"\"\n",
    "    return lsh_model.transform(df_feat)\n",
    "\n",
    "def show_signature_examples(df_with_hashes, n=5, cols=(\"id\", \"label_bin\", \"hashes\")):\n",
    "    df_with_hashes.select(*cols).show(n, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e1b9e0",
   "metadata": {},
   "source": [
    "### 2.1.3 Entrainer simplement chacun des 4 classeurs sans validation croisée. Le réglage des paramètres de l’algorithme approxNearestNeighbos sera uniquement sur le nombre de fonctions de hachages « numHashTables » du MinHash. Utiliser une plage de 2 valeurs maximum, par exemple {128, 250} afin de trouver la bonne longueur de la meilleure signature (réduction de dimensionnalité) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82515950",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HASH_LIST = (128, 250)\n",
    "\n",
    "# quand on est en local pour pas detruire mon ordi\n",
    "df_small = limit_dataset_size(df_label_bin, ratio=0.001)\n",
    "\n",
    "#Features (les 4 scénarios)\n",
    "df_s1 = make_s1_words(df_small)\n",
    "df_s2 = make_s2_ngrams(df_small)\n",
    "df_s3 = make_s3_patterns(df_small)\n",
    "df_s4 = make_s4_combo(df_small)\n",
    "\n",
    "#Entraînement\n",
    "models_s1, train_s1, test_s1,times_s1 = train_models_for_scenario(df_s1, numHashTables_list=NUM_HASH_LIST, train_ratio=0.8, cache=True,measure_time=True)\n",
    "models_s2, train_s2, test_s2,times_s2 = train_models_for_scenario(df_s2, numHashTables_list=NUM_HASH_LIST, train_ratio=0.8, cache=True,measure_time=True)\n",
    "models_s3, train_s3, test_s3,times_s3 = train_models_for_scenario(df_s3, numHashTables_list=NUM_HASH_LIST, train_ratio=0.8, cache=True,measure_time=True)\n",
    "models_s4, train_s4, test_s4,times_s4 = train_models_for_scenario(df_s4, numHashTables_list=NUM_HASH_LIST, train_ratio=0.8, cache=True,measure_time=True)\n",
    "\n",
    "print(\"OK: modèles entraînés pour S1..S4 avec numHashTables =\", NUM_HASH_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd81fe",
   "metadata": {},
   "source": [
    "### 2.1.4 Tester les classeurs obtenus et mesurer l’exactitude « accuracy » selon les différentes valeurs de k plus proches voisins : {50, 100, 150, 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87915d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rows = []\n",
    "results_rows += eval_scenario(models_s1, train_s1, test_s1, \"S1_words\")\n",
    "results_rows += eval_scenario(models_s2, train_s2, test_s2, \"S2_ngrams\")\n",
    "results_rows += eval_scenario(models_s3, train_s3, test_s3, \"S3_patterns\")\n",
    "results_rows += eval_scenario(models_s4, train_s4, test_s4, \"S4_combo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b95c49",
   "metadata": {},
   "source": [
    "### 2.1.5 Tracer le tableau des performances de classification, à l’image de la table 4 de l’article,selon les différents modèles de classeurs obtenus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab44ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = build_results_table(results_rows)     \n",
    "df_table4  = format_table4_like(df_results)\n",
    "df_table4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce063683",
   "metadata": {},
   "source": [
    "### 2.1.6 Tracer le graphique (histogramme) du temps d’entrainement, à l’image de la figure 2 en mesurant le temps d’entrainement pour les différents modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f165fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = pd.concat([\n",
    "    times_to_df(\"S1_words\", times_s1),\n",
    "    times_to_df(\"S2_ngrams\", times_s2),\n",
    "    times_to_df(\"S3_patterns\", times_s3),\n",
    "    times_to_df(\"S4_combo\", times_s4),\n",
    "], ignore_index=True)\n",
    "\n",
    "plot_training_histogram(df_time,group_by=(\"scenario\", \"numHashTables\"),value_col=\"train_time_seconds\")\n",
    "\n",
    "\n",
    "# S3 est le plus long, la raison pour la quelle s4 est pas tout aussi long c parce que le traitment a deja ete fait pour s3 donc il le refait pas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef8ffad",
   "metadata": {},
   "source": [
    "### 2.1.7 Tracer la courbe du passage à l’échelle -scalability-, à l’image de la figure 3 de l’article,pour chaque modèle entrainé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d885fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_list = (0.2, 0.4, 0.6, 0.8, 1.0)\n",
    "#en local:\n",
    "#F_list = (0.01,0.05)\n",
    "\n",
    "rows = []\n",
    "\n",
    "S_all = [\n",
    "            (\"S1_words\", df_s1),\n",
    "            (\"S2_ngrams\", df_s2),\n",
    "            (\"S3_patterns\", df_s3),\n",
    "            (\"S4_combo\", df_s4),\n",
    "        ]\n",
    "\n",
    "for scenario_name, df_feat in S_all:\n",
    "    for nht in NUM_HASH_LIST:\n",
    "        df_tmp = benchmark_scalability_vs_fraction(\n",
    "            df_feat=df_feat,\n",
    "            scenario_name=scenario_name,\n",
    "            numHashTables=nht,\n",
    "            F_list=F_list,\n",
    "            cache=True\n",
    "        )\n",
    "        rows.append(df_tmp)\n",
    "\n",
    "df_scaling = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "plot_scalability_fraction(df_scaling, hue=\"scenario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cf1da4",
   "metadata": {},
   "source": [
    "### 2.1.9 Afficher quelques exemples de signatures obtenues pour chaque classeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37917801",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXAMPLES = 2\n",
    "NHT = NUM_HASH_LIST[0]\n",
    "\n",
    "S_all_model_data = [\n",
    "    (\"S1_words\", models_s1, train_s1),\n",
    "    (\"S2_ngrams\", models_s2, train_s2),\n",
    "    (\"S3_patterns\", models_s3, train_s3),\n",
    "    (\"S4_combo\", models_s4, train_s4),\n",
    "]\n",
    "\n",
    "for scenario_name, models, train_df in S_all_model_data:\n",
    "    print(f\"\\n=== {scenario_name} | numHashTables={NHT} ===\")\n",
    "\n",
    "    lsh_model = models[NHT]\n",
    "\n",
    "    df_with_hashes = add_signatures(lsh_model, train_df)\n",
    "\n",
    "    show_signature_examples(\n",
    "        df_with_hashes,\n",
    "        n=N_EXAMPLES\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb0add",
   "metadata": {},
   "source": [
    "### 2.1.10 Pour pouvoir comparer avec les résultats de l’article, entrainer cette fois-ci, un classeur multi-classes en ajoutant la classe « neutre » tout en répétant les étapes précédentes : 2-1-2 à 2-1-9\n",
    "\n",
    "On construit le nouveau df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5121029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_propre = nettoie_df(df,True)\n",
    "df_mc = build_df_label_mc(df_propre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4a14f",
   "metadata": {},
   "source": [
    "On commence par refaire le 2.1.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HASH_LIST = (128, 250)\n",
    "LABEL_NAME = \"label_mc\"\n",
    "# Sous-échantillonnage (local)\n",
    "df_small_mc = limit_dataset_size(df_mc, ratio=0.001)\n",
    "\n",
    "df_s1_mc = make_s1_words(df_small_mc,LABEL_NAME)\n",
    "df_s2_mc = make_s2_ngrams(df_small_mc,label_name=LABEL_NAME)\n",
    "df_s3_mc = make_s3_patterns(df_small_mc,label_name=LABEL_NAME)\n",
    "df_s4_mc = make_s4_combo(df_small_mc,label_name=LABEL_NAME)\n",
    "\n",
    "models_s1_mc, train_s1_mc, test_s1_mc, times_s1_mc = train_models_for_scenario(df_s1_mc, numHashTables_list=NUM_HASH_LIST, train_ratio=0.8, cache=True, measure_time=True)\n",
    "models_s2_mc, train_s2_mc, test_s2_mc, times_s2_mc = train_models_for_scenario(df_s2_mc, numHashTables_list=NUM_HASH_LIST, train_ratio=0.8, cache=True, measure_time=True)\n",
    "models_s3_mc, train_s3_mc, test_s3_mc, times_s3_mc = train_models_for_scenario(df_s3_mc, numHashTables_list=NUM_HASH_LIST, train_ratio=0.8, cache=True, measure_time=True)\n",
    "models_s4_mc, train_s4_mc, test_s4_mc, times_s4_mc = train_models_for_scenario(df_s4_mc, numHashTables_list=NUM_HASH_LIST, train_ratio=0.8, cache=True, measure_time=True)\n",
    "\n",
    "print(\"OK: modèles multi-classes entraînés pour S1..S4 avec numHashTables =\", NUM_HASH_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ef6d2",
   "metadata": {},
   "source": [
    "Ensuite le 2.1.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_mc = []\n",
    "rows_mc += eval_scenario(models_s1_mc, train_s1_mc, test_s1_mc, \"S1_words_mc\", label_name=LABEL_NAME)\n",
    "rows_mc += eval_scenario(models_s2_mc, train_s2_mc, test_s2_mc, \"S2_ngrams_mc\", label_name=LABEL_NAME)\n",
    "rows_mc += eval_scenario(models_s3_mc, train_s3_mc, test_s3_mc, \"S3_patterns_mc\", label_name=LABEL_NAME)\n",
    "rows_mc += eval_scenario(models_s4_mc, train_s4_mc, test_s4_mc, \"S4_combo_mc\", label_name=LABEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e278e",
   "metadata": {},
   "source": [
    "2.1.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533460a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = build_results_table(rows_mc)     \n",
    "df_table4  = format_table4_like(df_results)\n",
    "df_table4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebeb8c1",
   "metadata": {},
   "source": [
    "2.1.6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = pd.concat([\n",
    "    times_to_df(\"S1_words\", times_s1),\n",
    "    times_to_df(\"S2_ngrams\", times_s2),\n",
    "    times_to_df(\"S3_patterns\", times_s3),\n",
    "    times_to_df(\"S4_combo\", times_s4),\n",
    "], ignore_index=True)\n",
    "\n",
    "plot_training_histogram(df_time,group_by=(\"scenario\", \"numHashTables\"),value_col=\"train_time_seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376dfba4",
   "metadata": {},
   "source": [
    "2.1.7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad25524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#F_list = (0.2, 0.4, 0.6, 0.8, 1.0)\n",
    "#en local:\n",
    "F_list = (0.01,0.05)\n",
    "\n",
    "rows = []\n",
    "\n",
    "S_all = [\n",
    "            (\"S1_words\", df_s1),\n",
    "            (\"S2_ngrams\", df_s2),\n",
    "            (\"S3_patterns\", df_s3),\n",
    "            (\"S4_combo\", df_s4),\n",
    "        ]\n",
    "\n",
    "for scenario_name, df_feat in S_all:\n",
    "    for nht in NUM_HASH_LIST:\n",
    "        df_tmp = benchmark_scalability_vs_fraction(\n",
    "            df_feat=df_feat,\n",
    "            scenario_name=scenario_name,\n",
    "            numHashTables=nht,\n",
    "            F_list=F_list,\n",
    "            cache=True\n",
    "        )\n",
    "        rows.append(df_tmp)\n",
    "\n",
    "df_scaling = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "plot_scalability_fraction(df_scaling, hue=\"scenario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fefffd7",
   "metadata": {},
   "source": [
    "2.1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3dc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXAMPLES = 2\n",
    "NHT = NUM_HASH_LIST[0]\n",
    "\n",
    "S_all_model_data = [\n",
    "    (\"S1_words\", models_s1, train_s1),\n",
    "    (\"S2_ngrams\", models_s2, train_s2),\n",
    "    (\"S3_patterns\", models_s3, train_s3),\n",
    "    (\"S4_combo\", models_s4, train_s4),\n",
    "]\n",
    "\n",
    "for scenario_name, models, train_df in S_all_model_data:\n",
    "    print(f\"\\n=== {scenario_name} | numHashTables={NHT} ===\")\n",
    "    lsh_model = models[NHT]\n",
    "    df_with_hashes = add_signatures(lsh_model, train_df)\n",
    "    show_signature_examples(df_with_hashes,n=N_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba03b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_scaling[[\"scenario\", \"numHashTables\", \"F\", \"time_seconds\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa1ed6c",
   "metadata": {},
   "source": [
    "### 2.1.11 Une fois l’étude (2.1.1 à 2.1.10) est réalisée et démontrée, choisissez un des classeurs (binaire ou ternaire) pour intégrer un filtre de Bloom à la place d’un HashingTF. Vous pouvez réutiliser n’importe quel code trouvé sur le net pour implémenter le filtre de Bloom ! À condition qu’il faut le comprendre et le commenter\n",
    "\n",
    "On choisit le scenario 1 c'est le moins lourd et le plus simple\n",
    "\n",
    "BloomFilter source: https://www.youtube.com/watch?v=V3pzxngeLqw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62dbca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "import hashlib\n",
    "\n",
    "def bloom_vector_from_tokens(tokens, m, k):\n",
    "    \"\"\"\n",
    "    Construction d’un filtre de Bloom à partir d’une liste de tokens.\n",
    "\n",
    "    Principe (selon la vidéo) :\n",
    "    - Le filtre de Bloom est un tableau de bits de taille m (bit array).\n",
    "    - Pour chaque élément (token), on applique k fonctions de hachage.\n",
    "    - Chaque fonction de hachage retourne une position dans [0, m-1].\n",
    "    - Les bits correspondants sont mis à 1.\n",
    "    - Des collisions sont possibles (faux positifs), mais aucun faux négatif.\n",
    "\n",
    "    Adaptation au contexte Spark :\n",
    "    - Le tableau de bits est représenté sous forme de SparseVector.\n",
    "    - Seules les positions à 1 sont stockées.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cas limite : aucun token\n",
    "    if tokens is None:\n",
    "        # Vecteur binaire vide de taille m\n",
    "        return Vectors.sparse(m, [], [])\n",
    "\n",
    "    # Ensemble des indices activés dans le filtre de Bloom\n",
    "    idx = set()\n",
    "\n",
    "    # Pour chaque token du document\n",
    "    for t in tokens:\n",
    "        if not t:\n",
    "            continue\n",
    "\n",
    "        # Application de k fonctions de hachage\n",
    "        # (simulées ici par le même hash avec des seeds différentes)\n",
    "        for i in range(k):\n",
    "            # Calcul d’un hash stable à partir du token et du seed i\n",
    "            # sha256 est utilisé pour garantir un hash déterministe\n",
    "            h = hashlib.sha256((t + \"#\" + str(i)).encode(\"utf-8\")).digest()\n",
    "\n",
    "            # Conversion des 8 premiers octets en entier 64 bits\n",
    "            # puis projection dans l’intervalle [0, m-1]\n",
    "            pos = int.from_bytes(h[:8], byteorder=\"little\", signed=False) % m\n",
    "\n",
    "            # Activation du bit correspondant\n",
    "            idx.add(int(pos))\n",
    "\n",
    "    # Conversion en SparseVector :\n",
    "    # - idx : positions mises à 1\n",
    "    # - valeurs : 1.0 pour chaque position active\n",
    "    idx = sorted(idx)\n",
    "    return Vectors.sparse(m, idx, [1.0] * len(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b89486",
   "metadata": {},
   "source": [
    "On recréer le scénario 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac05aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "\n",
    "def make_s1_words_bloom_spark(df, label_name=\"label_bin\", m=1<<18, k=7):\n",
    "    # 1) Tokenisation (comme S1)\n",
    "    tok = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens_raw\", pattern=\"\\\\W+\", toLowercase=True)\n",
    "    sw  = StopWordsRemover(inputCol=\"tokens_raw\", outputCol=\"tokens\")\n",
    "    df2 = sw.transform(tok.transform(df))\n",
    "\n",
    "    # 2) Bloom \"façon vidéo\" : positions = hash(token + i) % m\n",
    "    # On génère des \"pseudo-tokens\" représentant les bits activés : \"p_<pos>\"\n",
    "    # explode tokens, cross join avec i in [0..k-1], calc pos, regroupe par doc.\n",
    "    seeds = spark.range(0, k).withColumnRenamed(\"id\", \"seed\")  # 0..k-1\n",
    "\n",
    "    df_bits = (df2\n",
    "        .select(\"id\", F.col(label_name).alias(label_name), F.explode(F.col(\"tokens\")).alias(\"t\"))\n",
    "        .crossJoin(seeds)\n",
    "        .withColumn(\"pos\", (F.pmod(F.hash(F.concat_ws(\"#\", F.col(\"t\"), F.col(\"seed\").cast(\"string\"))), F.lit(m))))\n",
    "        .withColumn(\"bit_token\", F.concat(F.lit(\"p_\"), F.col(\"pos\").cast(\"string\")))\n",
    "        .groupBy(\"id\", label_name)\n",
    "        .agg(F.collect_set(\"bit_token\").alias(\"bloom_bits\"))\n",
    "    )\n",
    "\n",
    "    # 3) Bit array -> SparseVector via CountVectorizer (binary=True)\n",
    "    # vocabSize = m ici serait énorme (262k), CountVectorizer gère, mais peut être lourd.\n",
    "    # On laisse vocabSize grand, mais tu peux réduire m pour local.\n",
    "    cv = CountVectorizer(inputCol=\"bloom_bits\", outputCol=\"features\", binary=True, vocabSize=m, minDF=1.0)\n",
    "\n",
    "    cv_model = cv.fit(df_bits)\n",
    "    out = cv_model.transform(df_bits).select(\"id\", label_name, \"features\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4599c99",
   "metadata": {},
   "source": [
    "On fait l'entraînement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df1d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s1_bloom = make_s1_words_bloom_spark(df_small, label_name=\"label_bin\", m=1<<16, k=7)\n",
    "\n",
    "models_s1_bloom, train_s1_bloom, test_s1_bloom, times_s1_bloom = train_models_for_scenario(\n",
    "    df_s1_bloom,\n",
    "    numHashTables_list=NUM_HASH_LIST,\n",
    "    train_ratio=0.8,\n",
    "    cache=True,\n",
    "    measure_time=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4bece1",
   "metadata": {},
   "source": [
    "Evaluation du bloom et normale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc2c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_bloom = eval_scenario(\n",
    "    models_s1_bloom, train_s1_bloom, test_s1_bloom,\n",
    "    scenario_name=\"S1_words_BLOOM\",\n",
    "    label_name=\"label_bin\"\n",
    ")\n",
    "\n",
    "df_results_bloom = pd.DataFrame(rows_bloom)\n",
    "df_results_bloom\n",
    "\n",
    "rows_hash = eval_scenario(\n",
    "    models_s1, train_s1, test_s1,\n",
    "    scenario_name=\"S1_words_HASHINGTF\",\n",
    "    label_name=\"label_bin\"\n",
    ")\n",
    "\n",
    "df_results_hash = pd.DataFrame(rows_hash)\n",
    "df_results_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84995629",
   "metadata": {},
   "source": [
    "Comparasion plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8751ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_vs_k(df_results, title=\"\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    for (sc, nht), g in df_results.groupby([\"scenario\", \"numHashTables\"]):\n",
    "        g2 = g.sort_values(\"k\")\n",
    "        plt.plot(g2[\"k\"], g2[\"acc\"], marker=\"o\", label=f\"{sc} | nht={nht}\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_acc_vs_k(pd.concat([df_results_hash, df_results_bloom], ignore_index=True),title=\"S1 : HashingTF vs Bloom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e76863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_bloom = eval_scenario(\n",
    "    models_s1_bloom, train_s1_bloom, test_s1_bloom,\n",
    "    scenario_name=\"S1_words_BLOOM\",\n",
    "    label_name=\"label_bin\"\n",
    ")\n",
    "\n",
    "df_results_bloom = pd.DataFrame(rows_bloom)\n",
    "df_results_bloom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os, sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "python_path = sys.executable\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = python_path\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = python_path\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "        .config(\"spark.pyspark.python\", \"python\")\n",
    "        .config(\"spark.pyspark.driver.python\", \"python\")\n",
    "        \n",
    "         .config(\"spark.sql.execution.pyspark.udf.faulthandler.enabled\", \"true\")\n",
    "         .config(\"spark.python.worker.faulthandler.enabled\", \"true\")\n",
    "         .getOrCreate())\n",
    "\n",
    "print(\"Driver python:\", sys.executable)\n",
    "print(\"PYSPARK_PYTHON:\", os.environ[\"PYSPARK_PYTHON\"])\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A_A_D_M",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
